---
title: "Predicting poisonous mushroom from morphologocal characteristics"
author: "Group 4 - Dongxiao Li, Kyle Maj, Mahmoodur Rahman"
date: "2020/11/26 (updated: `r Sys.Date()`)"
always_allow_html: true
output:
  html_document:
    keep_md: true
    toc: true
bibliography: refs.bib
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
library(knitr)
library(kableExtra)
library(tidyverse)
```

```{r load tables, include=FALSE, warning=FALSE}
cf_tr_tab_1 <- read_csv("../results/cv_score.csv")
cf_tr_tab_2 <- read_csv("../results/lr_confusion_matrix.csv")
cf_test_tab_3 <- read_csv("../results/confusion_matrix_test.csv")
```

# Summary

As mushrooms have distinctive characteristics which help in identifying whether they are poisons or edible. In this project we have built a logistic regression classification model which can use several morphological characteristics of mushrooms to predict whether an observed mushroom is toxic or edible (non-toxic). Exploratory data analysis revealed definite distinctions between our target classes, as well as highlighting several key patterns which could serve as strong predictors. On the test data set of 1,625 observations our model performed extremely well with a 99% recall score and a 100% precision score. The model correctly classified 863 edible and 761 toxic mushrooms. One false negative result was produced (toxic mushroom identified as non-toxic). In the context of this problem, a false negative could result in someone being seriously or even fatally poisoned. We must therefore be far more concerned with minimizing false negatives than false positives. Given this precedent, we may consider tuning the threshold of our model in order to minimize false negatives at the potential cost of increasing false positives. Moving forward, we would like to further optimize our model, investigating if we could potentially get similar performance with less features. Finally, we would like to evaluate how our model performs on real observations from the field rather than hypothetical data.

# Introduction

Mushrooms are species of fungus, of which some can be eaten with meaty texture. However, some types are toxic [@TEGZES2002397]. Annually, a significant number of people die from ingesting poisonous mushrooms [@lei2016mushroom, @white2019mushroom]. The BC Centre for Disease Control (BCCDC) received 200 calls relating to mushroom poisoning in 2018 [@bccdc_mush]. Thus, It is critical to recognize a mushroom of poisonous species by observing it's appearance. Appearance primarily refers to specific physical characteristics. A model recognizing mushroom toxicity by taking these physical traits into account can be effective at preventing instances of mushroom poisoning [@diaz2005evolving]. Recent methods on classifying mushrooms fall into four groups: chemical determination, animal experimentation, fungal classification and folk experience [@fukuwatari2001establishment]. These methods are not perfect and there is room for improvement [@min2006present]. Mankind has been identifying toxic mushrooms by observing morphology, smell and distinct features for some time [@tanaka1996histopathological]. These intuitive-based methods are less reliable, and often lead to fatal incidents. However, relying on these experiences and intuitions, machine-learning models can be tried and tested. In this era of fourth Industrial revolution, artificial intelligence is playing a major role through deployment of machine-learning and deep learning models [@reynolds1965mushrooms]. This extends to classifying poisonous mushrooms, and several models have already been developed. Chaoqun and colleagues developed an android-based application, which detects toxic mushrooms through machine-learning models [@chaoqun]. To further improve classification, decision fusion method has been used, by stacking algorithms [@zhifeng]. Shuaichang and colleagues used image-based models for poisonous mushroom detection [@Shuaichang].

# Methods

## Data

For this project we are using the "Mushroom" dataset from UC Irvine's Machine learning repository. The data set was originally donated on April 27, 1987 and has since been cited 76 times. The data set contains hypothetical samples of 23 species of mushrooms classified in the Agaricus and Lepiota Families. There were originally three classes in our target feature: poisonous, edible, and unknown. For simplicity, all 'unknown' mushrooms are assumed to be poisonous. The original data set can be found [here](https://archive-beta.ics.uci.edu/ml/datasets/mushroom). Our only modifications to this data set were to drop the veil-type feature (which only had one class) and use imputation to replace missing values in the stalk-root feature. The data was processed through the tidyverse package [@hadley]; Exploratory data analysis was plotted using ggplot2 [@hadley_gg]. This report was compiled using an R [@r_cite] and Python [@van1995python] and R markdown [@rmarkdown] with knitr [@knitr] package document file with scripts running via the docopt [@docopt] package.

## Analysis

### Exploration

In this project, we first randomly split the raw data file into a train dataset(80%) and a test dataset(20%). Tabular and visual exploratory analysis were then conducted on the train data set. By doing exploratory analysis, we identified which features might be more useful to predict the classification target. We also examined the distribution of toxic and non-toxic mushroom across the categorical features in the training data set.

```{r eda, echo=FALSE, fig.cap="Figure 1.Distribution of Target feature (Target = 1: Toxic, and Target = 0: Edible) in the training set", out.width='100%'}
knitr::include_graphics("../results/eda_plot.png")
```

From exploratory data analysis we can see that edible mushrooms are likely to have have sunken(denoted as s) cap-shape, green(r) or purple(u) cap-color, red(e) or orange(o) gill color, and brown(n) and orange(o) veil color. They also have rooted stalk root(r), are of flaring ring type(f), and black(b), orange(o), purple(u) or yellow(y) spore print color. Another characteristics of edible mushroom is they are abundant(a) or numerous(n) in population, and dwells in waste(w) type habitat.

We also noticed that the feature veil type only has one class 'p' so we will have to drop this feature when doing the model fitting. Also, there are missing values in stalk root that we need to deal with using column transformer to impute proper values in.

```{r cap, echo=FALSE, fig.cap="Figure 2.Different mushroom cap shape", out.width='50%'}
knitr::include_graphics("../results/img/cap_shape.png")
```

### Prediction

We used the Sklearn LogisticRegression [@pedregosa2011scikit] algorithm to create a classification model which predicts whether a mushroom was poisonous or edible (found in the `class` column of the data set).

```{r confusion-matrix_1, echo=FALSE}
kable(cf_tr_tab_1, 
      col.names = c("","Baseline (DummyClassifier)", "LogisticRegression"),
      caption = "Table 1. Table of cross-validation score results for models used") |> 
  kable_styling(full_width = FALSE) 
```

We decided to use LogisticRregression classifier from the scikit-learn pacakge. To better understand the performance of our selected models, we use the scoring metrics of accuracy, precision, recall, f1, roc_auc and average precision when doing cross validation. The cross-validation scores for each model are summarized in the `Table 1`.

In order to benchmark our LogisticRegression classifier, we also used DummyClassifier to build a baseline model. We discovered that LogisticRegression returned extremely high cross-validation scores on all scoring metrics we used. With the exception of recall, the other metrics scored 1 which is the highest score a model can reach.

```{r confusion-matrix_2, echo=FALSE}
kable(cf_tr_tab_2, 
      col.names = c("True / Predicted","Edible(Non-toxic)", "Poisonous(Toxic)"),
      caption = "Table 2. Confusion Matrix of Prediction Given by Cross Validation") |> 
  kable_styling(full_width = FALSE) 
```

These results are visualized in the confusion matrix of prediction in cross validation (Table 2). The vertical side states the true observed values and the horizontal side states the predicted values. We can see that the TN(True Negative) is 3345 such that for mushrooms that are edible, our Logistic Regression Classifier identified all of them correctly as 'edible' since 'edible' is the negative class in our model. FP(False Positive, or type I error) here is 0, indicating that all edible mushrooms are being correctly identified. TP(True Positive) is 3152, for 3154 poisonous mushrooms in our train data set, 3152 of them are correctly identified as 'poisonous' while FN(False Negative, or type II error) is 2 indicating that 2 observations were identified as 'edible' even though they are actually poisonous.

We can see how the $0.99$ recall score in the Table 1 is calculated given the below formula for `recall` :

`recall = TP / (TP+FN) = 3152 / (3152+2) = 0.99`

Given that our model is performing much better than we expected, We did not do hyper-parameter optimization and directly applied it to the test data set to evaluate our model. The confusion matrix of prediction on the test data is given below (Table 3).

The test result is quite similar to the cross validation results. We can see that the TN(True Negative) is 863. Indicating that our Logistic Regression Classifier correctly identified all 'edible' mushrooms. FP(False Positive) here is 0 given all the edible mushrooms are being correctly identified. TP(True Positive) is 761, indicating that of 762 poisonous mushrooms in our test data set, 761 of them are correctly identified as 'poisonous'. FN(False Negative) is 1 since 1 observation is classified as 'edible' even though it is actually poisonous.

```{r confusion-matrix_3, echo=FALSE}
kable(cf_test_tab_3, 
      col.names = c("True / Predicted","Edible(Non-toxic)", "Poisonous(Toxic)"),
      caption = "Table 3. Confusion Matrix of Prediction on Test Data") |> 
  kable_styling(full_width = FALSE) 
```

# Limitations & Assumptions

With a recall score of 0.99 while the remaining scoring metrics are 1, our LogicsticRegression classifier is performing quite well in predicting whether a mushroom is poisonous or edible. Given the unexpected performance and high accuracy of our model, we decided to do sanity checks from expert's advice. First, we examined out data set for target class imbalance. The train data set contains 3334 edible mushrooms and 3154 poisonous mushrooms which are quite equally distributed. Also in the test data set, there are 863 edible mushrooms and 762 poisonous mushrooms. This confirmed that existing target class imbalance is nowhere near large enough to cause significant distortion in our results. Additionally, we checked if our data set is too small, resulting in unrealistic accuracy scores. Given this suggestion, the train data set contains 6499 observations which is sufficient for model training and fitting. WE do acknowledge that even with detection of 1 false negative among 762 poisonous mushrooms, may result to lethal consequences, and we will have to see how the model performs with real world data before deployment.

An additional concern that we would like to examine in future refinements is the possibility of overfitting due to feature class imbalance. Unlike the target class, there are a large number of features in the data set with significant class imbalance. As we expand on this project we intend to perform a more comprehensive analysis of feature importance and selection in order to address this issue. That being said, we do not believe there is clear evidence of overfitting in the current model as train and test scores are both high, but do not have a significant gap between them.

In conclusion, we believe the results of our model to be reliable. But there are still some potential improvements we can made which will be discussed in the next section.

# Looking Forward

In the future there are two key areas we would like to explore. Firstly, we would like to conduct an analysis of feature importance to gain insight into which features were most critical in predicting mushroom toxicity. This could potentially allow us to drop several features while preserving model performance. With less features the model will not only be less expensive computationally, but the potential costs of gathering additional data could also be drastically reduced. Secondly, we would like to test our model on a real data set. While our model performs remarkably well on the hypothetical samples of UC Irvine's Mushroom data set we cannot be fully confident until it has been tested on real-world data. Ideally we will be able to find more existing data sets with similar features. If not, it may be necessary to conduct our own sampling in the field.

# References
